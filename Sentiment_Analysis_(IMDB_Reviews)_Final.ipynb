import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

df=pd.read_csv('IMDB Dataset.csv')

df

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax


MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

df=df[df['review'].notna()]

tp=0
fp=0
tn=0
fn=0

nltk.download('punkt')

for j in range(100):
  if (len(nltk.word_tokenize(df['review'][j])))>350:
    x=polarity_scores_roberta(' '.join(nltk.word_tokenize(df['review'][j])[:350]))
  else:
    x=polarity_scores_roberta(df['review'][j])
  neg=x['roberta_neg']
  pos=x['roberta_pos']
  if(neg>pos):
    sentiment='negative'
  else:
    sentiment='positive'
  sentiment_hat=df['sentiment'][j]
  if(sentiment=='negative' and sentiment_hat=='negative'):
    tn+=1
  if(sentiment=='negative' and sentiment_hat=='positive'):
    fp+=1
  if(sentiment=='positive' and sentiment_hat=='negative'):
    fn+=1
  if(sentiment=='positive' and sentiment_hat=='positive'):
    tp+=1
  print(str(j)+" "+sentiment+" , "+sentiment_hat)

prec=tp/(tp+fp)
rec=tp/(tp+fn)

f1=2*prec*rec/(prec+rec)

f1

[tp,tn,fp,fn]

accuracy= (tp+tn)/(tp+tn+fp+fn)*100

print("Accuracy: "+str(accuracy)+"%")

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
def calculate_confusion_matrix(tp, fp, fn, tn):
    confusion_matrix = [[tp, fp], [fn, tn]]
    return confusion_matrix

def calculate_precision(tp, fp):
    if tp + fp == 0:
        return 0
    else:
        precision = tp / (tp + fp)
        return precision

def calculate_recall(tp, fn):
    if tp + fn == 0:
        return 0
    else:
        recall = tp / (tp + fn)
        return recall
confusion_matrix = calculate_confusion_matrix(tp, fp, fn, tn)
precision = calculate_precision(tp, fp)
recall = calculate_recall(tp, fn)

print("Confusion Matrix:")
print(confusion_matrix)
print("Precision:", precision)
print("Recall:", recall)

!pip install sentencepiece

from transformers import XLNetTokenizer, TFXLNetForSequenceClassification
import tensorflow as tf
import numpy as np

# Load pretrained XLNet model and tokenizer
model_name = "xlnet-base-cased"
model = TFXLNetForSequenceClassification.from_pretrained(model_name)
tokenizer = XLNetTokenizer.from_pretrained(model_name)


def XL(statement):

  # Tokenize the statement
  tokenized_statement = tokenizer(statement, return_tensors="tf", padding=True, truncation=True)

  # Convert BatchEncoding to a dictionary of NumPy arrays
  tokenized_statement_dict = {key: np.array(value) for key, value in tokenized_statement.items()}

  # Make prediction
  logits = model.predict(tokenized_statement_dict)[0]
  predicted_class = tf.argmax(logits, axis=1).numpy()[0]

  #   Print the predicted sentiment
  sentiment = "positive" if predicted_class == 1 else "negative"
  return sentiment

for j in range(100):
  sentiment=XL(df['review'][j])
  sentiment_hat=df['sentiment'][j]
  if(sentiment=='negative' and sentiment_hat=='negative'):
    tn+=1
  if(sentiment=='negative' and sentiment_hat=='positive'):
    fp+=1
  if(sentiment=='positive' and sentiment_hat=='negative'):
    fn+=1
  if(sentiment=='positive' and sentiment_hat=='positive'):
    tp+=1
  print(str(j)+" "+sentiment+" , "+sentiment_hat)

prec=tp/(tp+fp)
rec=tp/(tp+fn)

f1=2*prec*rec/(prec+rec)

f1

[tp,tn,fp,fn]

accuracy= (tp+tn)/(tp+tn+fp+fn)*100

print("Accuracy: "+str(accuracy)+"%")

confusion_matrix = calculate_confusion_matrix(tp, fp, fn, tn)
precision = calculate_precision(tp, fp)
recall = calculate_recall(tp, fn)

print("Confusion Matrix:")
print(confusion_matrix)
print("Precision:", precision)
print("Recall:", recall)

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load pretrained BERT model and tokenizer for sentiment analysis
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)



def BERT(statement):

  # Example statement

  # Tokenize the statement
  tokenized_statement = tokenizer(statement, return_tensors="pt", padding=True, truncation=True)

  # Make prediction
  with torch.no_grad():
    logits = model(**tokenized_statement).logits

  # Convert logits to probabilities
  probabilities = torch.nn.functional.softmax(logits, dim=1)

  # Get the predicted sentiment class (0 to 4, corresponding to very negative to very positive)
  predicted_class = torch.argmax(probabilities, dim=1).item()

  # Print the predicted sentiment
  sentiment_mapping = {0: 'negative', 1: 'negative', 2: 'positive', 3: 'positive', 4: 'positive'}
  predicted_sentiment = sentiment_mapping[predicted_class]
  return predicted_sentiment

correct=0
c=0
j=0
for h in df['review']:
  rating=BERT(df['review'][j])
  rating_hat=df['sentiment'][j]
  if(rating==rating_hat):
    correct+=1
  c+=1
  j+=1
  if(j==100):
    break
  print([rating,rating_hat])

Accuracy=(correct/c)*100

print("Accuracy: "+str(Accuracy)+"%")

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

# Load pretrained DistilBERT model and tokenizer for sentiment analysis
model_name = "assemblyai/distilbert-base-uncased-sst2"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name)

def distilBERT(statement):
  tokenized_statement = tokenizer(statement, return_tensors="pt", padding=True, truncation=True)

  # Make prediction
  with torch.no_grad():
    logits = model(**tokenized_statement).logits

  # Convert logits to probabilities
  probabilities = torch.nn.functional.softmax(logits, dim=1)

  # Get the predicted sentiment class (0 for negative, 1 for positive)
  predicted_class = torch.argmax(probabilities, dim=1).item()

  # Print the predicted sentiment
  sentiment_mapping = {0: 'negative', 1: 'positive'}
  predicted_sentiment = sentiment_mapping[predicted_class]

  return predicted_sentiment

correct=0
c=0
j=0
df_list = []
for h in df['review']:
  rating=distilBERT(df['review'][j])
  rating_hat=df['sentiment'][j]
  if(rating==rating_hat):
    correct+=1
  c+=1
  j+=1
  if(j==100):
    break
  print([rating,rating_hat])
  df_list.append([rating,rating_hat])

Accuracy=(correct/c)*100

print("Accuracy: "+str(Accuracy)+"%")

from transformers import GPT2Tokenizer, GPT2ForSequenceClassification
import torch

# Load pretrained GPT-2 model and tokenizer for sentiment analysis
model_name = "michelecafagna26/gpt2-medium-finetuned-sst2-sentiment"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2ForSequenceClassification.from_pretrained(model_name)



def GPT2(statement):
  tokenized_statement = tokenizer(statement, return_tensors="pt", padding=True, truncation=True)

# Make prediction
  with torch.no_grad():
    logits = model(**tokenized_statement).logits

# Convert logits to probabilities
  probabilities = torch.nn.functional.softmax(logits, dim=1)

# Get the predicted sentiment class (0 for negative, 1 for positive)
  predicted_class = torch.argmax(probabilities, dim=1).item()

# Print the predicted sentiment
  sentiment_mapping = {0: 'negative', 1: 'positive'}
  predicted_sentiment = sentiment_mapping[predicted_class]

  return predicted_sentiment

correct=0
c=0
j=0
df_list = []
for h in df['review']:
  rating=GPT2(df['review'][j])
  rating_hat=df['sentiment'][j]
  if(rating==rating_hat):
    correct+=1
  c+=1
  j+=1
  if(j==100):
    break
  print([rating,rating_hat])
  df_list.append([rating,rating_hat])

Accuracy=(correct/c)*100

print("Accuracy: "+str(Accuracy)+"%")

df = pd.DataFrame(df_list,columns=["category_encoded","predicted_sentiment"])

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
y_true = df['category_encoded']
y_pred = df['predicted_sentiment']

# Calculate and print the confusion matrix
conf_mat = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(conf_mat)

# Calculate and print other metrics
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy}")

# Print classification report which includes precision, recall, and F1 score
class_report = classification_report(y_true, y_pred)
print("Classification Report:")
print(class_report)

# Accuracies of various models:

*   RoBERTa -- **87.00%**
*   XLnet -- **71.00%**
*   BERT -- **84.00%**
*   DistilBERT -- **91.00%**
*   GPT2 -- **90.00%**

